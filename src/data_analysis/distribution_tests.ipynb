{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d7d828",
   "metadata": {},
   "source": [
    "# Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.data_constants import JORDAN_DATASET_FILEPATH, MAESTRO_DATASET_FILEPATH\n",
    "from constants.real_time_constants import SLIDING_WINDOW_LEN, STRIDE\n",
    "from data.jordan_dataset import JordanDataset\n",
    "from data.maestro_dataset import MaestroDataset\n",
    "from data.sliding_window import SlidingWindowDataset\n",
    "\n",
    "## pure data\n",
    "id_train_base_dataset = JordanDataset(\n",
    "    data_dir=JORDAN_DATASET_FILEPATH,\n",
    "    split=\"train\",\n",
    "    name=\"id_train_dataset\",\n",
    "    split_input_and_output_ids=True,\n",
    ")\n",
    "id_test_base_dataset = JordanDataset(\n",
    "    data_dir=JORDAN_DATASET_FILEPATH,\n",
    "    split=\"validation\",\n",
    "    name=\"id_test_dataset\",\n",
    "    split_input_and_output_ids=True,\n",
    ")\n",
    "ood_test_base_dataset = MaestroDataset(\n",
    "    data_dir=MAESTRO_DATASET_FILEPATH,\n",
    "    split=\"test\",\n",
    "    name=\"maestro_test_dataset\"\n",
    ")\n",
    "\n",
    "## dataset that takes chunks of 120 tokens out of the above datasets\n",
    "id_train_dataset = SlidingWindowDataset(\n",
    "    base_dataset=id_train_base_dataset,\n",
    "    name=\"id_train_dataset\",\n",
    "    k=SLIDING_WINDOW_LEN,\n",
    "    stride=STRIDE,\n",
    ")\n",
    "id_test_dataset = SlidingWindowDataset(\n",
    "    base_dataset=id_test_base_dataset,\n",
    "    name=\"id_test_dataset\",\n",
    "    k=SLIDING_WINDOW_LEN,\n",
    "    stride=STRIDE,\n",
    ")   \n",
    "ood_test_dataset = SlidingWindowDataset(\n",
    "    base_dataset=ood_test_base_dataset,\n",
    "    name=\"ood_test_dataset\",\n",
    "    k=SLIDING_WINDOW_LEN,\n",
    "    stride=STRIDE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.model_constants import JORDAN_MODEL_NAME, DEVICE\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM\n",
    "from utils.data_loading import collate_fn\n",
    "from extract_layers.pooling_functions import pool_last_k_tokens\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(JORDAN_MODEL_NAME).to(DEVICE)\n",
    "num_layers = 24\n",
    "pooling_function = pool_last_k_tokens(1)\n",
    "\n",
    "id_train_dataloader = DataLoader(\n",
    "    id_train_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_layers.extract_layers_main import extract_representations\n",
    "\n",
    "data = extract_representations(\n",
    "    model,\n",
    "    id_train_dataloader,\n",
    "    pooling_function,\n",
    "    layers=[12],\n",
    ")\n",
    "layer_data = data[12]\n",
    "print(layer_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387eea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to numpy if it's a torch tensor\n",
    "if hasattr(layer_data, 'numpy'):\n",
    "    X = layer_data.numpy()\n",
    "else:\n",
    "    X = np.array(layer_data)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Fit PCA with maximum components to see full explained variance\n",
    "n_components = min(100, X.shape[1])  # Use up to 100 components or all features\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X)\n",
    "\n",
    "# Explained variance ratio (proportion of total variance explained by each component)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  # Shape: (n_components,)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Find number of components for 95% explained variance\n",
    "# np.searchsorted finds the index where 0.95 would be inserted to maintain sorted order\n",
    "# This gives us the first index where cumulative_variance >= 0.95\n",
    "n_components_95 = np.searchsorted(cumulative_variance, 0.95) + 1\n",
    "n_components_99 = np.searchsorted(cumulative_variance, 0.99) + 1\n",
    "# If threshold is never reached, searchsorted returns len(array), so we cap it\n",
    "n_components_95 = min(n_components_95, len(cumulative_variance))\n",
    "n_components_99 = min(n_components_99, len(cumulative_variance))\n",
    "\n",
    "print(f\"\\nExplained Variance Statistics:\")\n",
    "print(f\"  First component: {explained_variance_ratio[0]:.4f} ({explained_variance_ratio[0]*100:.2f}%)\")\n",
    "print(f\"  First 10 components: {cumulative_variance[9]:.4f} ({cumulative_variance[9]*100:.2f}%)\")\n",
    "print(f\"  Components for 95% variance: {n_components_95}\")\n",
    "print(f\"  Components for 99% variance: {n_components_99}\")\n",
    "print(f\"  Total variance explained by {n_components} components: {cumulative_variance[-1]:.4f} ({cumulative_variance[-1]*100:.2f}%)\")\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Explained variance ratio per component\n",
    "axes[0].plot(range(1, min(50, len(explained_variance_ratio)) + 1), \n",
    "              explained_variance_ratio[:50], 'b-', marker='o', markersize=3)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Explained Variance per Component')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), \n",
    "             cumulative_variance, 'r-', linewidth=2)\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "axes[1].axhline(y=0.99, color='orange', linestyle='--', label='99% threshold')\n",
    "axes[1].axvline(x=n_components_95, color='g', linestyle=':', alpha=0.5)\n",
    "axes[1].axvline(x=n_components_99, color='orange', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP visualization in 2D and 3D (with fallback to t-SNE if UMAP unavailable)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Try to import UMAP, fall back to t-SNE if there's a compatibility issue\n",
    "try:\n",
    "    import umap\n",
    "    USE_UMAP = True\n",
    "    method_name = \"UMAP\"\n",
    "except ImportError as e:\n",
    "    print(f\"UMAP import failed ({e}), falling back to t-SNE...\")\n",
    "    from sklearn.manifold import TSNE\n",
    "    USE_UMAP = False\n",
    "    method_name = \"t-SNE\"\n",
    "\n",
    "# Subsample data (it's computationally expensive)\n",
    "# Use a subset for faster computation\n",
    "n_samples = min(5000, X.shape[0])\n",
    "if X.shape[0] > n_samples:\n",
    "    print(f\"Subsampling to {n_samples} samples for {method_name} visualization...\")\n",
    "    indices = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "    X_subset = X[indices]\n",
    "else:\n",
    "    X_subset = X\n",
    "    indices = np.arange(X.shape[0])\n",
    "\n",
    "print(f\"Running {method_name} on {X_subset.shape[0]} samples...\")\n",
    "\n",
    "# 2D embedding\n",
    "print(f\"Computing 2D {method_name}...\")\n",
    "if USE_UMAP:\n",
    "    reducer_2d = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "else:\n",
    "    reducer_2d = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "result_2d = reducer_2d.fit_transform(X_subset)\n",
    "\n",
    "# 3D embedding\n",
    "print(f\"Computing 3D {method_name}...\")\n",
    "if USE_UMAP:\n",
    "    reducer_3d = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "else:\n",
    "    reducer_3d = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=1000)\n",
    "result_3d = reducer_3d.fit_transform(X_subset)\n",
    "\n",
    "# Plot embeddings\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# 2D plot\n",
    "ax1 = fig.add_subplot(121)\n",
    "scatter = ax1.scatter(result_2d[:, 0], result_2d[:, 1], \n",
    "                     c=range(len(result_2d)), cmap='viridis', \n",
    "                     alpha=0.6, s=10)\n",
    "ax1.set_xlabel(f'{method_name} Component 1')\n",
    "ax1.set_ylabel(f'{method_name} Component 2')\n",
    "ax1.set_title(f'{method_name} Visualization (2D)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax1, label='Sample Index')\n",
    "\n",
    "# 3D plot\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "scatter3d = ax2.scatter(result_3d[:, 0], result_3d[:, 1], result_3d[:, 2],\n",
    "                       c=range(len(result_3d)), cmap='viridis', \n",
    "                       alpha=0.6, s=10)\n",
    "ax2.set_xlabel(f'{method_name} Component 1')\n",
    "ax2.set_ylabel(f'{method_name} Component 2')\n",
    "ax2.set_zlabel(f'{method_name} Component 3')\n",
    "ax2.set_title(f'{method_name} Visualization (3D)')\n",
    "plt.colorbar(scatter3d, ax=ax2, label='Sample Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{method_name} completed!\")\n",
    "print(f\"2D {method_name} shape: {result_2d.shape}\")\n",
    "print(f\"3D {method_name} shape: {result_3d.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc7026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
